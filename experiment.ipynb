{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen Distillation Lab (System 1 + System 2)\n",
        "\n",
        "**Colab-ready notebook** to distill **Qwen2.5-7B-Instruct** into two smaller students:\n",
        "- **System 1** — instruction-following (7B → 0.5B) via black-box KD on DistilQwen_100k\n",
        "- **System 2** — reasoning / chain-of-thought (1.5B) via SFT-style KD on OmniThought\n",
        "\n",
        "Uses `distill_app.py` for data prep and EasyDistill for training. **Run cells in order** (or *Run all*); **GPU runtime recommended**. Colab: open from GitHub so the repo is cloned; local: open the notebook from the repo root.\n",
        "\n",
        "**Important:** Run the **\"Project root and imports\"** cell before any Prepare / Run Distillation / Test / Comparison cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0️⃣ Runtime setup\n",
        "\n",
        "Confirm GPU and Python. In Colab: **Runtime → Change runtime type → GPU** (e.g. T4) before running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!nvidia-smi\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣ Install dependencies\n",
        "\n",
        "Core libs + **EasyDistill from source**. Clone EasyDistill so the `easydistill` CLI and templates are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TPU setup (optional, for faster training)\n",
        "\n",
        "If you chose **TPU** runtime (Runtime → Change runtime type → TPU), run this cell once. It installs PyTorch/XLA so teacher labeling and student training run on TPU. **Note:** EasyDistill's vllm-based teacher inference is GPU-only; on TPU we use our own path (HF generate for teacher + in-notebook TPU training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect TPU: old runtimes use COLAB_TPU_ADDR, v4+/v5e/v6e use PJRT (/dev/accel0)\n",
        "_has_tpu_env = bool(os.environ.get(\"COLAB_TPU_ADDR\"))\n",
        "_has_pjrt_dev = Path(\"/dev/accel0\").exists()\n",
        "_has_tpu_name = bool(os.environ.get(\"TPU_NAME\"))\n",
        "\n",
        "if _has_tpu_env or _has_pjrt_dev or _has_tpu_name:\n",
        "    print(f\"TPU detected (COLAB_TPU_ADDR={_has_tpu_env}, /dev/accel0={_has_pjrt_dev}, TPU_NAME={_has_tpu_name})\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torch_xla[tpu]\", \"-f\", \"https://storage.googleapis.com/libtpu-releases/index.html\"], check=False)\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        dev = xm.xla_device()\n",
        "        print(f\"TPU ready. XLA device: {dev}\")\n",
        "    except Exception as e:\n",
        "        print(\"torch_xla import issue:\", e)\n",
        "        print(\"Trying torch_xla.runtime...\")\n",
        "        try:\n",
        "            import torch_xla.runtime as xr\n",
        "            print(f\"TPU device count: {xr.global_runtime_device_count()}\")\n",
        "        except Exception as e2:\n",
        "            print(\"torch_xla.runtime also failed:\", e2)\n",
        "else:\n",
        "    print(\"No TPU detected. Using GPU path.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU detected (COLAB_TPU_ADDR not set). Using GPU path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q \"torch>=2.1.0\" \"transformers>=4.36.0\" \"datasets>=2.16.0\" \"accelerate>=0.25.0\" \"sentencepiece>=0.1.99\"\n",
        "%pip install -q bitsandbytes>=0.43.0 tqdm nltk rouge-score jsonlines \"trl>=0.7.0\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"/content\").exists():\n",
        "    EASYDIR = Path(\"/content/easydistill\")\n",
        "else:\n",
        "    EASYDIR = Path.cwd() / \"easydistill\"\n",
        "\n",
        "if not EASYDIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/modelscope/easydistill.git\", str(EASYDIR)], check=True)\n",
        "r = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(EASYDIR)], capture_output=True, text=True)\n",
        "if r.returncode != 0:\n",
        "    print(\"Standard install failed:\")\n",
        "    print(r.stderr or r.stdout or \"(no output)\")\n",
        "    print(\"Trying fallback: install with --no-deps, then requirements...\")\n",
        "    r2 = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(EASYDIR), \"--no-deps\"], capture_output=True, text=True)\n",
        "    if r2.returncode != 0:\n",
        "        print(\"Fallback --no-deps also failed:\", r2.stderr or r2.stdout)\n",
        "        raise SystemExit(r.returncode)\n",
        "    req = EASYDIR / \"requirements.txt\"\n",
        "    if req.exists():\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req)], check=False)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"vllm\"], check=False)\n",
        "print(\"EasyDistill installed from\", EASYDIR)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EasyDistill installed from /content/easydistill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clone repo (if needed)\n",
        "\n",
        "If you got FileNotFoundError above (e.g. opened from Drive/upload): run the code cell below once, then re-run the Project root and imports cell. Set GITHUB_REPO to your fork if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace YOUR_USERNAME with your GitHub username\n",
        "GITHUB_REPO = \"https://github.com/zacharias1219/distilled-model-research.git\"\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "if Path(\"/content\").exists() and not (Path(\"/content/distilled-model-research\") / \"distill_app.py\").exists():\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, \"/content/distilled-model-research\"], check=True)\n",
        "    import os\n",
        "    os.chdir(\"/content/distilled-model-research\")\n",
        "    print(\"Cloned. Now re-run the 'Project root and imports' cell above.\")\n",
        "else:\n",
        "    print(\"Not in Colab or repo already present. If you still see FileNotFoundError, run locally from the repo root.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloned. Now re-run the 'Project root and imports' cell above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HF token (optional)\n",
        "\n",
        "**Colab (open from GitHub):** Add `HF_TOKEN` in Colab Secrets (key icon in the left sidebar) so Hugging Face uses it for auth and higher rate limits. Run this cell once.\n",
        "\n",
        "**Local:** If you have a `.env` in the repo root with `HF_TOKEN=...`, it is loaded when you import `distill_app` below; no need to do anything here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "    print(\"HF_TOKEN set from Colab secrets.\")\n",
        "except Exception:\n",
        "    pass  # Local or no secret: .env will be used when distill_app is imported"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project root and imports\n",
        "\n",
        "Ensure we're in the repo root (where `distill_app.py` lives). In Colab from GitHub, repo is usually `/content/distilled-model-research`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_project_root():\n",
        "    if Path(\"/content\").exists():\n",
        "        for d in Path(\"/content\").iterdir():\n",
        "            if d.is_dir() and (d / \"distill_app.py\").exists():\n",
        "                return d\n",
        "    for p in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        if (p / \"distill_app.py\").exists():\n",
        "            return p\n",
        "    return Path.cwd()\n",
        "\n",
        "ROOT = _find_project_root()\n",
        "if ROOT != Path.cwd():\n",
        "    import os\n",
        "    os.chdir(ROOT)\n",
        "    print(\"Working directory:\", ROOT)\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "if not (ROOT / \"distill_app.py\").exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"distill_app.py not found. Colab (Drive/upload): run the 'Clone repo (if needed)' cell below, then re-run this cell. \"\n",
        "        \"Local: run this notebook from the repo root (the folder that contains distill_app.py).\"\n",
        "    )\n",
        "\n",
        "from distill_app import (\n",
        "    load_teacher,\n",
        "    prepare_system1_dataset,\n",
        "    prepare_system2_dataset,\n",
        "    distill_system1,\n",
        "    distill_system2,\n",
        "    compare_models,\n",
        "    load_student,\n",
        "    infer_student,\n",
        "    format_prompt,\n",
        "    find_checkpoint,\n",
        "    evaluate_student,\n",
        ")\n",
        "print(\"distill_app imported from\", ROOT)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distill_app imported from /content/distilled-model-research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports are in the \"Project root and imports\" cell above. Skip this cell."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2️⃣ System 1: Instruction-following distillation (7B → 0.5B)\n",
        "\n",
        "Load a subset of **DistilQwen_100k**, optionally re-label with the teacher, then run black-box KD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config\n",
        "\n",
        "Increase `DATASET_SLICE_SYS1` (e.g. `train[:5000]`) or `NUM_EPOCHS_SYS1` for better quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TEACHER_MODEL_SYS1 = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "STUDENT_MODEL_SYS1 = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "DATASET_SLICE_SYS1 = \"train[:1000]\"\n",
        "NUM_EPOCHS_SYS1 = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data & Label (Optional)\n",
        "\n",
        "Loads DistilQwen_100k, maps to `{instruction, input, output}`. Set `RELABEL_WITH_TEACHER = True` to re-generate outputs with the teacher (slower, more VRAM).\n",
        "\n",
        "**Note:** HF Hub may show warnings about `HF_TOKEN` / unauthenticated requests. You can ignore them; downloads still work. For higher rate limits, add `HF_TOKEN` in Colab secrets (key icon in the sidebar) and run `from huggingface_hub import login; login()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RELABEL_WITH_TEACHER = False  # Set True to re-label with teacher (requires loading teacher first)\n",
        "\n",
        "teacher_sys1 = None\n",
        "tokenizer_sys1 = None\n",
        "if RELABEL_WITH_TEACHER:\n",
        "    teacher_sys1, tokenizer_sys1 = load_teacher(TEACHER_MODEL_SYS1)\n",
        "\n",
        "prepare_system1_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS1,\n",
        "    teacher_model=teacher_sys1,\n",
        "    teacher_tokenizer=tokenizer_sys1,\n",
        "    relabel_with_teacher=RELABEL_WITH_TEACHER,\n",
        "    out_instructions=\"data/train_instructions.json\",\n",
        "    out_labeled=\"data/train_labeled.json\",\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "607629e907714a349e1c79618210dae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3543a582efb542b8b3e0ea3598f4cf18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/124M [00:00<?, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c34713d968694121a28e4dbfed9324b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saved System 1 instructions to data/train_instructions.json\n",
            "Saved System 1 labeled data to data/train_labeled.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Distillation\n",
        "\n",
        "Calls EasyDistill (black-box KD). Checkpoint will be written to `./distilled-qwen2.5-0.5b` (or the path you set in config)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config_sys1 = {\n",
        "    \"teacher_model\": TEACHER_MODEL_SYS1,\n",
        "    \"student_model\": STUDENT_MODEL_SYS1,\n",
        "    \"labeled_path\": \"data/train_labeled.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS1,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-0.5b\",\n",
        "    \"config_path\": \"configs/kd_black_box_qwen_0_5b.json\",\n",
        "    \"template_path\": None,\n",
        "}\n",
        "\n",
        "# If EasyDistill was cloned, point to its template (configs/chat_template/chat_template_kd.jinja)\n",
        "if Path(\"/content\").exists() and Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = \"/content/easydistill/configs/chat_template/chat_template_kd.jinja\"\n",
        "elif (Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = str(Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\")\n",
        "\n",
        "path_sys1 = distill_system1(config_sys1)\n",
        "if path_sys1:\n",
        "    print(\"Final checkpoint path:\", path_sys1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e045c5834984efda69a5cb27a7a6bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4c98ba9ac0f48bcab923a86140b9f4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76811e665cb246509680c62a092e10c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a082a7199f41434398a90570f74fc33d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Wrote System 1 KD config to configs/kd_black_box_qwen_0_5b.json\n",
            "Running: /usr/bin/python3 -m easydistill.cli --config /content/distilled-model-research/configs/kd_black_box_qwen_0_5b.json\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -            ^^^^^^^^^^^^^^^^^^^\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\", line 247, in __init__\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -     self.llm_engine = LLMEngine.from_engine_args(\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 503, in from_engine_args\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -     vllm_config = engine_args.create_engine_config(usage_context)\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1273, in create_engine_config\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -     config = VllmConfig(\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -              ^^^^^^^^^^^\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -   File \"<string>\", line 19, in __init__\n",
            "[stderr] 2026-03-01 12:14:26,634 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 3851, in __post_init__\n",
            "[stderr] 2026-03-01 12:14:26,635 - INFO -     self.model_config.verify_with_parallel_config(self.parallel_config)\n",
            "[stderr] 2026-03-01 12:14:26,635 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 933, in verify_with_parallel_config\n",
            "[stderr] 2026-03-01 12:14:26,635 - INFO -     if total_num_attention_heads % tensor_parallel_size != 0:\n",
            "[stderr] 2026-03-01 12:14:26,635 - INFO -        ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n",
            "[stderr] 2026-03-01 12:14:26,635 - INFO - ZeroDivisionError: integer modulo by zero\n",
            "[stderr] 2026-03-01 12:14:26,635 - ERROR - Detected error in output: ZeroDivisionError: integer modulo by zero\n",
            "[stderr] 2026-03-01 12:14:27,728 - ERROR - Command failed (returncode=1, errors detected)\n",
            "[stderr] 2026-03-01 12:14:27,728 - ERROR - Infer failed, skipping training\n",
            "EasyDistill run FAILED (returncode=0 + fatal errors detected in stderr).\n",
            "\n",
            "EasyDistill CLI failed. Falling back to direct SFT training...\n",
            "(This uses pre-existing labels from the dataset instead of teacher inference.)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import trl.trainer.sft_config because of the following error (look up to see its traceback):\nFailed to import transformers.training_args because of the following error (look up to see its traceback):\n/usr/local/lib/python3.12/dist-packages/_XLAC.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5torch4lazy13MetricFnValueB5cxx11Ed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_pt_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcceleratorConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mis_torch_xla_available\u001b[0;34m(check_is_tpu, check_is_gpu)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0m_XLAC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.12/dist-packages/_XLAC.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5torch4lazy13MetricFnValueB5cxx11Ed",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.training_args because of the following error (look up to see its traceback):\n/usr/local/lib/python3.12/dist-packages/_XLAC.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5torch4lazy13MetricFnValueB5cxx11Ed",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6068/1598478886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig_sys1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"template_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"easydistill\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"configs\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template_kd.jinja\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpath_sys1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistill_system1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final checkpoint path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/distilled-model-research/distill_app.py\u001b[0m in \u001b[0;36mdistill_system1\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEasyDistill CLI failed. Falling back to direct SFT training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(This uses pre-existing labels from the dataset instead of teacher inference.)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         code = run_training_sft(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0mlabeled_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabeled_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/distilled-model-research/distill_app.py\u001b[0m in \u001b[0;36mrun_training_sft\u001b[0;34m(labeled_path, student_model, out_dir, num_epochs, max_length, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trl not installed. Run: pip install trl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import trl.trainer.sft_config because of the following error (look up to see its traceback):\nFailed to import transformers.training_args because of the following error (look up to see its traceback):\n/usr/local/lib/python3.12/dist-packages/_XLAC.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5torch4lazy13MetricFnValueB5cxx11Ed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!cat debug-5b3ceb.log"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"COLAB_TPU_ADDR check\", \"data\": {\"value\": null}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367245510}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"torch_xla NOT installed\", \"data\": {}, \"hypothesisId\": \"H3\", \"timestamp\": 1772367245514}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:distill_system1\", \"message\": \"path selection\", \"data\": {\"tpu_detected\": false, \"cuda_available\": false}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367245514}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 1 student\n",
        "\n",
        "Load the distilled model and run a few prompts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    _base = ROOT\n",
        "except NameError:\n",
        "    _base = Path.cwd()\n",
        "\n",
        "# Use find_checkpoint to locate the actual model files\n",
        "student_path_sys1 = find_checkpoint(str(_base / \"distilled-qwen2.5-0.5b\"))\n",
        "if not student_path_sys1 and \"path_sys1\" in dir() and path_sys1:\n",
        "    student_path_sys1 = find_checkpoint(path_sys1)\n",
        "\n",
        "if student_path_sys1:\n",
        "    print(\"Loading checkpoint:\", student_path_sys1)\n",
        "    student_sys1, tok_sys1 = load_student(student_path_sys1)\n",
        "    for p in [\n",
        "        \"Explain what a large language model is to a high school student.\",\n",
        "        \"Write a Python function to check if a number is prime.\",\n",
        "        \"Give me three use cases of knowledge distillation in deep learning.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 1):\", infer_student(student_sys1, tok_sys1, p, mode=\"system1\", max_new_tokens=256))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found at distilled-qwen2.5-0.5b/\")\n",
        "    print(\"Run System 1 distillation first.\")\n",
        "    # Diagnostic info\n",
        "    _p = _base / \"distilled-qwen2.5-0.5b\"\n",
        "    if _p.exists():\n",
        "        print(f\"Directory exists but contains: {[f.name for f in _p.iterdir()][:20]}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint not found at distilled-qwen2.5-0.5b/\n",
            "Run System 1 distillation first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate System 1 student\n",
        "\n",
        "Compute perplexity, BLEU, and ROUGE-L on a held-out sample from the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if student_path_sys1 and 'student_sys1' in dir():\n",
        "    from distill_app import read_json, evaluate_student\n",
        "    # Use last 50 items from labeled data as eval set\n",
        "    _eval_data = read_json(\"data/train_labeled.json\")[-50:]\n",
        "    print(f\"Evaluating System 1 on {len(_eval_data)} held-out samples...\")\n",
        "    eval_results_sys1 = evaluate_student(\n",
        "        student_sys1, tok_sys1, _eval_data, mode=\"system1\", max_new_tokens=256, max_eval=50\n",
        "    )\n",
        "else:\n",
        "    print(\"System 1 student not loaded. Run distillation and test cells first.\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System 1 student not loaded. Run distillation and test cells first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## System 2 Distillation (Reasoning / CoT)\n",
        "\n",
        "Train a CoT-capable student on OmniThought so it shows step-by-step reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STUDENT_MODEL_SYS2 = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "DATASET_SLICE_SYS2 = \"train[:2000]\"\n",
        "RV_MIN = 0.6\n",
        "CD_MIN = 0.6\n",
        "NUM_EPOCHS_SYS2 = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare CoT Data\n",
        "\n",
        "Load OmniThought, filter by RV/CD if present, map to `{instruction, output=cot}` and save to `data/omnithought_cot.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prepare_system2_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS2,\n",
        "    rv_min=RV_MIN,\n",
        "    cd_min=CD_MIN,\n",
        "    out_cot=\"data/omnithought_cot.json\",\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing System 2 dataset: collecting 2000 samples via streaming...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42346c014577445fbd872ebbede6d8b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a204fe55cc204866b67f82d584b7044c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/135 [00:00<?, ?it/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46c72669d1314318ad1201aaf4ac6269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/135 [00:00<?, ?it/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Streaming OmniThought:  39%|███▉      | 779/2000 [00:09<00:08, 141.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "First OmniThought sample keys: ['question', 'reasoning']\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Streaming OmniThought: 100%|█████████▉| 1999/2000 [00:09<00:00, 203.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Collected 2000 CoT samples via streaming.\n",
            "Saved System 2 CoT data to data/omnithought_cot.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run CoT Distillation\n",
        "\n",
        "Calls EasyDistill (kd_black_box_train_only). Checkpoint: `./distilled-qwen2.5-1.5b-cot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config_sys2 = {\n",
        "    \"student_model\": STUDENT_MODEL_SYS2,\n",
        "    \"cot_path\": \"data/omnithought_cot.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS2,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-1.5b-cot\",\n",
        "    \"config_path\": \"configs/kd_cot_qwen_1_5b.json\",\n",
        "}\n",
        "# Use EasyDistill template from clone (same as System 1)\n",
        "_tpl = Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\") if Path(\"/content\").exists() else Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\"\n",
        "if _tpl.exists():\n",
        "    config_sys2[\"template_path\"] = str(_tpl)\n",
        "\n",
        "path_sys2 = distill_system2(config_sys2)\n",
        "if path_sys2:\n",
        "    print(\"Final checkpoint path:\", path_sys2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c055f3cc0b465ea07eb17085abd680",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00d6b84f6a74653a1f914b6fe7c0609",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa56a318de4c43699719f6c202dfca3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d1483936c52455fae60ab7be6dc6cc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Wrote System 2 KD config to configs/kd_cot_qwen_1_5b.json\n",
            "Running: /usr/bin/python3 -m easydistill.cli --config /content/distilled-model-research/configs/kd_cot_qwen_1_5b.json\n",
            "[stderr] 2026-02-28 12:58:08,470 - INFO - Running command: accelerate launch --config_file /content/easydistill/configs/accelerate_config/muti_gpu.yaml /content/easydistill/easydistill/kd/train.py --config /content/distilled-model-research/configs/kd_cot_qwen_1_5b.json\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO - Traceback (most recent call last):\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: Traceback (most recent call last):\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     sys.exit(main())\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -              ^^^^^^\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     args.func(args)\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1266, in launch_command\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     deepspeed_launcher(args)\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 913, in deepspeed_launcher\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     raise ImportError(\"DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\")\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: raise ImportError(\"DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\")\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO - ImportError: DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: ImportError: DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\n",
            "[stderr] 2026-02-28 12:58:11,018 - ERROR - Command failed (returncode=1, errors detected)\n",
            "EasyDistill run completed successfully.\n",
            "WARNING: EasyDistill exited successfully but no model files found in ./distilled-qwen2.5-1.5b-cot\n",
            "Directory contents: (dir does not exist)\n",
            "Final checkpoint path: /content/distilled-model-research/distilled-qwen2.5-1.5b-cot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 2 (CoT) student\n",
        "\n",
        "Prompts include CoT instruction; responses should show step-by-step reasoning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    _base2 = ROOT\n",
        "except NameError:\n",
        "    _base2 = Path.cwd()\n",
        "\n",
        "# Use find_checkpoint to locate the actual model files\n",
        "student_path_sys2 = find_checkpoint(str(_base2 / \"distilled-qwen2.5-1.5b-cot\"))\n",
        "if not student_path_sys2 and \"path_sys2\" in dir() and path_sys2:\n",
        "    student_path_sys2 = find_checkpoint(path_sys2)\n",
        "\n",
        "if student_path_sys2:\n",
        "    print(\"Loading checkpoint:\", student_path_sys2)\n",
        "    student_sys2, tok_sys2 = load_student(student_path_sys2)\n",
        "    for p in [\n",
        "        \"A train travels 120 km in 2 hours. If it continues at the same speed, how far will it travel in 5 hours?\",\n",
        "        \"You flip a fair coin 3 times. What is the probability of getting exactly two heads?\",\n",
        "        \"Explain the difference between overfitting and underfitting with an example.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 2 CoT):\", infer_student(student_sys2, tok_sys2, p, mode=\"system2\", max_new_tokens=512))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found at distilled-qwen2.5-1.5b-cot/\")\n",
        "    print(\"Run System 2 distillation first.\")\n",
        "    _p = _base2 / \"distilled-qwen2.5-1.5b-cot\"\n",
        "    if _p.exists():\n",
        "        print(f\"Directory exists but contains: {[f.name for f in _p.iterdir()][:20]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate System 2 (CoT) student\n",
        "\n",
        "Compute perplexity, BLEU, and ROUGE-L on a held-out sample from the CoT data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if student_path_sys2 and 'student_sys2' in dir():\n",
        "    from distill_app import read_json, evaluate_student\n",
        "    # Use last 50 items from CoT data as eval set\n",
        "    _eval_data_cot = read_json(\"data/omnithought_cot.json\")[-50:]\n",
        "    print(f\"Evaluating System 2 on {len(_eval_data_cot)} held-out samples...\")\n",
        "    eval_results_sys2 = evaluate_student(\n",
        "        student_sys2, tok_sys2, _eval_data_cot, mode=\"system2\", max_new_tokens=512, max_eval=50\n",
        "    )\n",
        "else:\n",
        "    print(\"System 2 student not loaded. Run distillation and test cells first.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4️⃣ Teacher vs student comparison\n",
        "\n",
        "Side-by-side: **Prompt → Teacher | System 1 | System 2**. Missing checkpoints are skipped with a clear message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "COMPARE_PROMPTS = [\n",
        "    \"Explain what overfitting means.\",\n",
        "    \"What is the time complexity of binary search?\",\n",
        "    \"A train travels 120 km in 2 hours. What is its average speed?\",\n",
        "    \"Explain the concept of knowledge distillation and why it is useful.\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    _base = ROOT\n",
        "except NameError:\n",
        "    _base = Path.cwd()\n",
        "\n",
        "_s1 = next((p for p in [\n",
        "    path_sys1 if \"path_sys1\" in dir() and path_sys1 else None,\n",
        "    str(_base / \"distilled-qwen2.5-0.5b\"),\n",
        "] if p and Path(p).exists()), str(_base / \"distilled-qwen2.5-0.5b\"))\n",
        "\n",
        "_s2 = next((p for p in [\n",
        "    path_sys2 if \"path_sys2\" in dir() and path_sys2 else None,\n",
        "    str(_base / \"distilled-qwen2.5-1.5b-cot\"),\n",
        "] if p and Path(p).exists()), str(_base / \"distilled-qwen2.5-1.5b-cot\"))\n",
        "\n",
        "compare_models(\n",
        "    COMPARE_PROMPTS,\n",
        "    teacher_path=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    system1_path=_s1,\n",
        "    system2_path=_s2,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ce9b72e84dc478c82c62bda55c6a326",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba92eb1fa104f8daec21f82f724b72b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0d98d67da74439aab4c08c840805cdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80f1a06172134de0abba83d8ebf239c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "208ea219ec7b41aab336caffe047e573",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "WARNING:bitsandbytes.backends.cpu.ops:Failed to load CPU gemm_4bit_forward from kernels-community: No module named 'kernels'. Please make sure you already `pip install kernels` and the kernels >= 0.11.1\n",
            "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'npu', 'hpu', 'xpu', 'cuda', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Model not found at Qwen/Qwen2.5-7B-Instruct (None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'npu', 'hpu', 'xpu', 'cuda', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)\n",
            "Model not found at /content/distilled-model-research/distilled-qwen2.5-0.5b\n",
            "Model not found at /content/distilled-model-research/distilled-qwen2.5-1.5b-cot\n",
            "\n",
            "============================================================\n",
            "[Prompt 1] Explain what overfitting means.\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 2] What is the time complexity of binary search?\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 3] A train travels 120 km in 2 hours. What is its average speed?\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 4] Explain the concept of knowledge distillation and why it is useful.\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Teacher vs System 2 CoT (side-by-side)\n",
        "\n",
        "Compare teacher and System 2 student on reasoning prompts with CoT-style prompting. Loads teacher and student if not already in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "COT_COMPARE = [\n",
        "    \"A bag has 3 red balls and 2 blue balls. If you draw two without replacement, what is the probability both are red?\",\n",
        "    \"What is the derivative of x^3 + 2x^2 - 5x + 7? Explain the steps.\",\n",
        "]\n",
        "if Path(\"./distilled-qwen2.5-1.5b-cot\").exists():\n",
        "    try:\n",
        "        _t, _tt = load_teacher(\"Qwen/Qwen2.5-7B-Instruct\")\n",
        "        _s2, _ts2 = load_student(\"./distilled-qwen2.5-1.5b-cot\")\n",
        "        for p in COT_COMPARE:\n",
        "            print(\"#\" * 72)\n",
        "            print(\"Prompt:\", p)\n",
        "            print(\"\\n[Teacher CoT]\", infer_student(_t, _tt, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print(\"\\n[Student CoT]\", infer_student(_s2, _ts2, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print()\n",
        "    except Exception as e:\n",
        "        print(\"Could not load models:\", e)\n",
        "else:\n",
        "    print(\"Run System 2 distillation first.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run System 2 distillation first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6️⃣ Evaluation Summary\n",
        "\n",
        "Side-by-side metrics for both distilled students."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Collect results from both evaluations\n",
        "_s1 = eval_results_sys1 if 'eval_results_sys1' in dir() else {}\n",
        "_s2 = eval_results_sys2 if 'eval_results_sys2' in dir() else {}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DISTILLATION EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<20} {'System 1 (0.5B)':>18} {'System 2 (1.5B CoT)':>20}\")\n",
        "print(\"-\" * 60)\n",
        "for metric in ['perplexity', 'bleu', 'rouge_l']:\n",
        "    v1 = _s1.get(metric, 'N/A')\n",
        "    v2 = _s2.get(metric, 'N/A')\n",
        "    print(f\"{metric:<20} {str(v1):>18} {str(v2):>20}\")\n",
        "n1 = _s1.get('num_evaluated', 0)\n",
        "n2 = _s2.get('num_evaluated', 0)\n",
        "print(f\"{'num_evaluated':<20} {str(n1):>18} {str(n2):>20}\")\n",
        "print(\"=\" * 60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5️⃣ Scaling up\n",
        "\n",
        "Once a small run works:\n",
        "- **Data:** Increase slices (e.g. `train[:10000]` System 1, `train[:5000]` System 2).\n",
        "- **Epochs:** Set `NUM_EPOCHS_SYS1` / `NUM_EPOCHS_SYS2` to 2–3.\n",
        "- **Batch size:** Increase in generated configs if VRAM allows.\n",
        "- **System 2 student:** Use `Qwen/Qwen2.5-0.5B-Instruct` if VRAM is tight.\n",
        "- **Relabeling:** Set `RELABEL_WITH_TEACHER = True` for teacher-generated labels (slower, often better)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick single-prompt inference\n",
        "\n",
        "Uncomment and run after you have a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model, tokenizer = load_student(\"./distilled-qwen2.5-0.5b\")\n",
        "# print(infer_student(model, tokenizer, \"Explain what overfitting means.\", mode=\"system1\"))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}