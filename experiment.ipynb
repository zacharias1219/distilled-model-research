{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen Distillation Lab (System 1 + System 2)\n",
        "\n",
        "**Colab-ready notebook** to distill **Qwen2.5-7B-Instruct** into two smaller students:\n",
        "- **System 1** — instruction-following (7B → 0.5B) via black-box KD on DistilQwen_100k\n",
        "- **System 2** — reasoning / chain-of-thought (1.5B) via SFT-style KD on OmniThought\n",
        "\n",
        "Uses `distill_app.py` for data prep and EasyDistill for training. **Run cells in order** (or *Run all*); **GPU runtime recommended**. Colab: open from GitHub so the repo is cloned; local: open the notebook from the repo root.\n",
        "\n",
        "**Important:** Run the **\"Project root and imports\"** cell before any Prepare / Run Distillation / Test / Comparison cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0️⃣ Runtime setup\n",
        "\n",
        "Confirm GPU and Python. In Colab: **Runtime → Change runtime type → GPU** (e.g. T4) before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Python 3.12.12\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣ Install dependencies\n",
        "\n",
        "Core libs + **EasyDistill from source**. Clone EasyDistill so the `easydistill` CLI and templates are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TPU setup (optional, for faster training)\n",
        "\n",
        "If you chose **TPU** runtime (Runtime → Change runtime type → TPU), run this cell once. It installs PyTorch/XLA so teacher labeling and student training run on TPU. **Note:** EasyDistill's vllm-based teacher inference is GPU-only; on TPU we use our own path (HF generate for teacher + in-notebook TPU training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPU detection: COLAB_TPU_ADDR=False, /dev/accel0=False, TPU_NAME=False, torch_xla_installed=True\n",
            "TPU runtime detected!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1802/3882931737.py:21: DeprecationWarning: Use torch_xla.device instead\n",
            "  dev = xm.xla_device()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch_xla works. XLA device: xla:0\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys, importlib.util\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect TPU: Colab TPU runtimes pre-install torch_xla; also check legacy indicators\n",
        "_has_tpu_env = bool(os.environ.get(\"COLAB_TPU_ADDR\"))\n",
        "_has_pjrt_dev = Path(\"/dev/accel0\").exists()\n",
        "_has_tpu_name = bool(os.environ.get(\"TPU_NAME\"))\n",
        "_has_xla_pkg = importlib.util.find_spec(\"torch_xla\") is not None\n",
        "\n",
        "IS_TPU_RUNTIME = _has_tpu_env or _has_pjrt_dev or _has_tpu_name or _has_xla_pkg\n",
        "\n",
        "print(f\"TPU detection: COLAB_TPU_ADDR={_has_tpu_env}, /dev/accel0={_has_pjrt_dev}, \"\n",
        "      f\"TPU_NAME={_has_tpu_name}, torch_xla_installed={_has_xla_pkg}\")\n",
        "\n",
        "if IS_TPU_RUNTIME:\n",
        "    print(\"TPU runtime detected!\")\n",
        "    # Colab ships matched torch + torch_xla — do NOT reinstall torch.\n",
        "    # Only reinstall torch_xla if the import is broken (ABI mismatch).\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        dev = xm.xla_device()\n",
        "        print(f\"torch_xla works. XLA device: {dev}\")\n",
        "    except Exception as e:\n",
        "        print(f\"torch_xla import failed: {e}\")\n",
        "        print(\"Reinstalling matched torch + torch_xla from libtpu index...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                        \"torch\", \"torch_xla[tpu]\",\n",
        "                        \"-f\", \"https://storage.googleapis.com/libtpu-releases/index.html\",\n",
        "                        \"--force-reinstall\", \"--no-deps\"], check=False)\n",
        "        # Also reinstall libtpu\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"libtpu\",\n",
        "                        \"-f\", \"https://storage.googleapis.com/libtpu-releases/index.html\"], check=False)\n",
        "        try:\n",
        "            import importlib\n",
        "            import torch_xla\n",
        "            importlib.reload(torch_xla)\n",
        "            import torch_xla.core.xla_model as xm\n",
        "            dev = xm.xla_device()\n",
        "            print(f\"After reinstall — torch_xla works. XLA device: {dev}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Still broken after reinstall: {e2}\")\n",
        "            print(\"You may need to restart the runtime (Runtime → Restart runtime) and re-run.\")\n",
        "else:\n",
        "    print(\"No TPU detected. Using GPU path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPU runtime: skipping torch install to preserve torch/torch_xla ABI match\n"
          ]
        }
      ],
      "source": [
        "import importlib.util as _ilu\n",
        "\n",
        "# On TPU runtimes, torch + torch_xla are pre-matched — do NOT upgrade torch\n",
        "if _ilu.find_spec(\"torch_xla\") is not None:\n",
        "    print(\"TPU runtime: skipping torch install to preserve torch/torch_xla ABI match\")\n",
        "    %pip install -q \"transformers>=4.36.0\" \"datasets>=2.16.0\" \"accelerate>=0.25.0\" \"sentencepiece>=0.1.99\"\n",
        "else:\n",
        "    %pip install -q \"torch>=2.1.0\" \"transformers>=4.36.0\" \"datasets>=2.16.0\" \"accelerate>=0.25.0\" \"sentencepiece>=0.1.99\"\n",
        "\n",
        "%pip install -q bitsandbytes>=0.43.0 tqdm nltk rouge-score jsonlines \"trl>=0.7.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EasyDistill installed from /content/easydistill\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"/content\").exists():\n",
        "    EASYDIR = Path(\"/content/easydistill\")\n",
        "else:\n",
        "    EASYDIR = Path.cwd() / \"easydistill\"\n",
        "\n",
        "if not EASYDIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/modelscope/easydistill.git\", str(EASYDIR)], check=True)\n",
        "r = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(EASYDIR)], capture_output=True, text=True)\n",
        "if r.returncode != 0:\n",
        "    print(\"Standard install failed:\")\n",
        "    print(r.stderr or r.stdout or \"(no output)\")\n",
        "    print(\"Trying fallback: install with --no-deps, then requirements...\")\n",
        "    r2 = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(EASYDIR), \"--no-deps\"], capture_output=True, text=True)\n",
        "    if r2.returncode != 0:\n",
        "        print(\"Fallback --no-deps also failed:\", r2.stderr or r2.stdout)\n",
        "        raise SystemExit(r.returncode)\n",
        "    req = EASYDIR / \"requirements.txt\"\n",
        "    if req.exists():\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req)], check=False)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"vllm\"], check=False)\n",
        "print(\"EasyDistill installed from\", EASYDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clone repo (if needed)\n",
        "\n",
        "If you got FileNotFoundError above (e.g. opened from Drive/upload): run the code cell below once, then re-run the Project root and imports cell. Set GITHUB_REPO to your fork if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloned. Now re-run the 'Project root and imports' cell above.\n"
          ]
        }
      ],
      "source": [
        "# Replace YOUR_USERNAME with your GitHub username\n",
        "GITHUB_REPO = \"https://github.com/zacharias1219/distilled-model-research.git\"\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "if Path(\"/content\").exists() and not (Path(\"/content/distilled-model-research\") / \"distill_app.py\").exists():\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, \"/content/distilled-model-research\"], check=True)\n",
        "    import os\n",
        "    os.chdir(\"/content/distilled-model-research\")\n",
        "    print(\"Cloned. Now re-run the 'Project root and imports' cell above.\")\n",
        "else:\n",
        "    print(\"Not in Colab or repo already present. If you still see FileNotFoundError, run locally from the repo root.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HF token (optional)\n",
        "\n",
        "**Colab (open from GitHub):** Add `HF_TOKEN` in Colab Secrets (key icon in the left sidebar) so Hugging Face uses it for auth and higher rate limits. Run this cell once.\n",
        "\n",
        "**Local:** If you have a `.env` in the repo root with `HF_TOKEN=...`, it is loaded when you import `distill_app` below; no need to do anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "    print(\"HF_TOKEN set from Colab secrets.\")\n",
        "except Exception:\n",
        "    pass  # Local or no secret: .env will be used when distill_app is imported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project root and imports\n",
        "\n",
        "Ensure we're in the repo root (where `distill_app.py` lives). In Colab from GitHub, repo is usually `/content/distilled-model-research`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distill_app imported from /content/distilled-model-research\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_project_root():\n",
        "    if Path(\"/content\").exists():\n",
        "        for d in Path(\"/content\").iterdir():\n",
        "            if d.is_dir() and (d / \"distill_app.py\").exists():\n",
        "                return d\n",
        "    for p in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        if (p / \"distill_app.py\").exists():\n",
        "            return p\n",
        "    return Path.cwd()\n",
        "    \n",
        "\n",
        "ROOT = _find_project_root()\n",
        "if ROOT != Path.cwd():\n",
        "    import os\n",
        "    os.chdir(ROOT)\n",
        "    print(\"Working directory:\", ROOT)\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "if not (ROOT / \"distill_app.py\").exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"distill_app.py not found. Colab (Drive/upload): run the 'Clone repo (if needed)' cell below, then re-run this cell. \"\n",
        "        \"Local: run this notebook from the repo root (the folder that contains distill_app.py).\"\n",
        "    )\n",
        "\n",
        "from distill_app import (\n",
        "    load_teacher,\n",
        "    prepare_system1_dataset,\n",
        "    prepare_system2_dataset,\n",
        "    distill_system1,\n",
        "    distill_system2,\n",
        "    compare_models,\n",
        "    load_student,\n",
        "    infer_student,\n",
        "    format_prompt,\n",
        "    find_checkpoint,\n",
        "    evaluate_student,\n",
        ")\n",
        "print(\"distill_app imported from\", ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports are in the \"Project root and imports\" cell above. Skip this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2️⃣ System 1: Instruction-following distillation (7B → 0.5B)\n",
        "\n",
        "Load a subset of **DistilQwen_100k**, optionally re-label with the teacher, then run black-box KD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config\n",
        "\n",
        "Increase `DATASET_SLICE_SYS1` (e.g. `train[:5000]`) or `NUM_EPOCHS_SYS1` for better quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEACHER_MODEL_SYS1 = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "STUDENT_MODEL_SYS1 = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "DATASET_SLICE_SYS1 = \"train[:1000]\"\n",
        "NUM_EPOCHS_SYS1 = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data & Label (Optional)\n",
        "\n",
        "Loads DistilQwen_100k, maps to `{instruction, input, output}`. Set `RELABEL_WITH_TEACHER = True` to re-generate outputs with the teacher (slower, more VRAM).\n",
        "\n",
        "**Note:** HF Hub may show warnings about `HF_TOKEN` / unauthenticated requests. You can ignore them; downloads still work. For higher rate limits, add `HF_TOKEN` in Colab secrets (key icon in the sidebar) and run `from huggingface_hub import login; login()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ad43fbd9ea9492aa9711564ba451df7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ed52ac4983b4cc4994c4e4d396b262a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/124M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f36566b8704fd68616ae37046d8a72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved System 1 instructions to data/train_instructions.json\n",
            "Saved System 1 labeled data to data/train_labeled.json\n"
          ]
        }
      ],
      "source": [
        "RELABEL_WITH_TEACHER = False  # Set True to re-label with teacher (requires loading teacher first)\n",
        "\n",
        "teacher_sys1 = None\n",
        "tokenizer_sys1 = None\n",
        "if RELABEL_WITH_TEACHER:\n",
        "    teacher_sys1, tokenizer_sys1 = load_teacher(TEACHER_MODEL_SYS1)\n",
        "\n",
        "prepare_system1_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS1,\n",
        "    teacher_model=teacher_sys1,\n",
        "    teacher_tokenizer=tokenizer_sys1,\n",
        "    relabel_with_teacher=RELABEL_WITH_TEACHER,\n",
        "    out_instructions=\"data/train_instructions.json\",\n",
        "    out_labeled=\"data/train_labeled.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Distillation\n",
        "\n",
        "Calls EasyDistill (black-box KD). Checkpoint will be written to `./distilled-qwen2.5-0.5b` (or the path you set in config)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPU detected: using TPU training path (no vllm).\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1802/1598478886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig_sys1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"template_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"easydistill\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"configs\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template_kd.jinja\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpath_sys1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistill_system1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final checkpoint path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/distilled-model-research/distill_app.py\u001b[0m in \u001b[0;36mdistill_system1\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtpu_detected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TPU detected: using TPU training path (no vllm).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         code = run_training_tpu(\n\u001b[0m\u001b[1;32m    946\u001b[0m             \u001b[0mlabeled_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabeled_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/distilled-model-research/distill_app.py\u001b[0m in \u001b[0;36mrun_training_tpu\u001b[0;34m(labeled_path, student_model, out_dir, num_epochs, max_length, learning_rate)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0msave_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0;31m     trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "config_sys1 = {\n",
        "    \"teacher_model\": TEACHER_MODEL_SYS1,\n",
        "    \"student_model\": STUDENT_MODEL_SYS1,\n",
        "    \"labeled_path\": \"data/train_labeled.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS1,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-0.5b\",\n",
        "    \"config_path\": \"configs/kd_black_box_qwen_0_5b.json\",\n",
        "    \"template_path\": None,\n",
        "}\n",
        "\n",
        "# If EasyDistill was cloned, point to its template (configs/chat_template/chat_template_kd.jinja)\n",
        "if Path(\"/content\").exists() and Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = \"/content/easydistill/configs/chat_template/chat_template_kd.jinja\"\n",
        "elif (Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = str(Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\")\n",
        "\n",
        "path_sys1 = distill_system1(config_sys1)\n",
        "if path_sys1:\n",
        "    print(\"Final checkpoint path:\", path_sys1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"TPU env check\", \"data\": {\"COLAB_TPU_ADDR\": null, \"pjrt_dev\": false, \"TPU_NAME\": null}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744797}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"torch_xla NOT installed\", \"data\": {\"pjrt_dev\": false}, \"hypothesisId\": \"H3\", \"timestamp\": 1772367744801}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:distill_system1\", \"message\": \"path selection\", \"data\": {\"tpu_detected\": false, \"cuda_available\": false}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744801}\n"
          ]
        }
      ],
      "source": [
        "!cat debug-5b3ceb.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"TPU env check\", \"data\": {\"COLAB_TPU_ADDR\": null, \"pjrt_dev\": false, \"TPU_NAME\": null}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744797}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"torch_xla NOT installed\", \"data\": {\"pjrt_dev\": false}, \"hypothesisId\": \"H3\", \"timestamp\": 1772367744801}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:distill_system1\", \"message\": \"path selection\", \"data\": {\"tpu_detected\": false, \"cuda_available\": false}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744801}\n"
          ]
        }
      ],
      "source": [
        "!cat debug-5b3ceb.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"TPU env check\", \"data\": {\"COLAB_TPU_ADDR\": null, \"pjrt_dev\": false, \"TPU_NAME\": null}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744797}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:_is_tpu\", \"message\": \"torch_xla NOT installed\", \"data\": {\"pjrt_dev\": false}, \"hypothesisId\": \"H3\", \"timestamp\": 1772367744801}\n",
            "{\"sessionId\": \"5b3ceb\", \"location\": \"distill_app.py:distill_system1\", \"message\": \"path selection\", \"data\": {\"tpu_detected\": false, \"cuda_available\": false}, \"hypothesisId\": \"H1\", \"timestamp\": 1772367744801}\n"
          ]
        }
      ],
      "source": [
        "!cat debug-5b3ceb.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 1 student\n",
        "\n",
        "Load the distilled model and run a few prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint not found at distilled-qwen2.5-0.5b/\n",
            "Run System 1 distillation first.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    _base = ROOT\n",
        "except NameError:\n",
        "    _base = Path.cwd()\n",
        "\n",
        "# Use find_checkpoint to locate the actual model files\n",
        "student_path_sys1 = find_checkpoint(str(_base / \"distilled-qwen2.5-0.5b\"))\n",
        "if not student_path_sys1 and \"path_sys1\" in dir() and path_sys1:\n",
        "    student_path_sys1 = find_checkpoint(path_sys1)\n",
        "\n",
        "if student_path_sys1:\n",
        "    print(\"Loading checkpoint:\", student_path_sys1)\n",
        "    student_sys1, tok_sys1 = load_student(student_path_sys1)\n",
        "    for p in [\n",
        "        \"Explain what a large language model is to a high school student.\",\n",
        "        \"Write a Python function to check if a number is prime.\",\n",
        "        \"Give me three use cases of knowledge distillation in deep learning.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 1):\", infer_student(student_sys1, tok_sys1, p, mode=\"system1\", max_new_tokens=256))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found at distilled-qwen2.5-0.5b/\")\n",
        "    print(\"Run System 1 distillation first.\")\n",
        "    # Diagnostic info\n",
        "    _p = _base / \"distilled-qwen2.5-0.5b\"\n",
        "    if _p.exists():\n",
        "        print(f\"Directory exists but contains: {[f.name for f in _p.iterdir()][:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate System 1 student\n",
        "\n",
        "Compute perplexity, BLEU, and ROUGE-L on a held-out sample from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System 1 student not loaded. Run distillation and test cells first.\n"
          ]
        }
      ],
      "source": [
        "if student_path_sys1 and 'student_sys1' in dir():\n",
        "    from distill_app import read_json, evaluate_student\n",
        "    # Use last 50 items from labeled data as eval set\n",
        "    _eval_data = read_json(\"data/train_labeled.json\")[-50:]\n",
        "    print(f\"Evaluating System 1 on {len(_eval_data)} held-out samples...\")\n",
        "    eval_results_sys1 = evaluate_student(\n",
        "        student_sys1, tok_sys1, _eval_data, mode=\"system1\", max_new_tokens=256, max_eval=50\n",
        "    )\n",
        "else:\n",
        "    print(\"System 1 student not loaded. Run distillation and test cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## System 2 Distillation (Reasoning / CoT)\n",
        "\n",
        "Train a CoT-capable student on OmniThought so it shows step-by-step reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "STUDENT_MODEL_SYS2 = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "DATASET_SLICE_SYS2 = \"train[:2000]\"\n",
        "RV_MIN = 0.6\n",
        "CD_MIN = 0.6\n",
        "NUM_EPOCHS_SYS2 = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare CoT Data\n",
        "\n",
        "Load OmniThought, filter by RV/CD if present, map to `{instruction, output=cot}` and save to `data/omnithought_cot.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing System 2 dataset: collecting 2000 samples via streaming...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42346c014577445fbd872ebbede6d8b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a204fe55cc204866b67f82d584b7044c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/135 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46c72669d1314318ad1201aaf4ac6269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/135 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming OmniThought:  39%|███▉      | 779/2000 [00:09<00:08, 141.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First OmniThought sample keys: ['question', 'reasoning']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming OmniThought: 100%|█████████▉| 1999/2000 [00:09<00:00, 203.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 2000 CoT samples via streaming.\n",
            "Saved System 2 CoT data to data/omnithought_cot.json\n"
          ]
        }
      ],
      "source": [
        "prepare_system2_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS2,\n",
        "    rv_min=RV_MIN,\n",
        "    cd_min=CD_MIN,\n",
        "    out_cot=\"data/omnithought_cot.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run CoT Distillation\n",
        "\n",
        "Calls EasyDistill (kd_black_box_train_only). Checkpoint: `./distilled-qwen2.5-1.5b-cot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49c055f3cc0b465ea07eb17085abd680",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00d6b84f6a74653a1f914b6fe7c0609",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa56a318de4c43699719f6c202dfca3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d1483936c52455fae60ab7be6dc6cc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote System 2 KD config to configs/kd_cot_qwen_1_5b.json\n",
            "Running: /usr/bin/python3 -m easydistill.cli --config /content/distilled-model-research/configs/kd_cot_qwen_1_5b.json\n",
            "[stderr] 2026-02-28 12:58:08,470 - INFO - Running command: accelerate launch --config_file /content/easydistill/configs/accelerate_config/muti_gpu.yaml /content/easydistill/easydistill/kd/train.py --config /content/distilled-model-research/configs/kd_cot_qwen_1_5b.json\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO - Traceback (most recent call last):\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: Traceback (most recent call last):\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     sys.exit(main())\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -              ^^^^^^\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     args.func(args)\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1266, in launch_command\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     deepspeed_launcher(args)\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -   File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 913, in deepspeed_launcher\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO -     raise ImportError(\"DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\")\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: raise ImportError(\"DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\")\n",
            "[stderr] 2026-02-28 12:58:10,653 - INFO - ImportError: DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\n",
            "[stderr] 2026-02-28 12:58:10,653 - ERROR - Detected error in output: ImportError: DeepSpeed is not installed => run `pip3 install deepspeed` or build it from source.\n",
            "[stderr] 2026-02-28 12:58:11,018 - ERROR - Command failed (returncode=1, errors detected)\n",
            "EasyDistill run completed successfully.\n",
            "WARNING: EasyDistill exited successfully but no model files found in ./distilled-qwen2.5-1.5b-cot\n",
            "Directory contents: (dir does not exist)\n",
            "Final checkpoint path: /content/distilled-model-research/distilled-qwen2.5-1.5b-cot\n"
          ]
        }
      ],
      "source": [
        "config_sys2 = {\n",
        "    \"student_model\": STUDENT_MODEL_SYS2,\n",
        "    \"cot_path\": \"data/omnithought_cot.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS2,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-1.5b-cot\",\n",
        "    \"config_path\": \"configs/kd_cot_qwen_1_5b.json\",\n",
        "}\n",
        "# Use EasyDistill template from clone (same as System 1)\n",
        "_tpl = Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\") if Path(\"/content\").exists() else Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\"\n",
        "if _tpl.exists():\n",
        "    config_sys2[\"template_path\"] = str(_tpl)\n",
        "\n",
        "path_sys2 = distill_system2(config_sys2)\n",
        "if path_sys2:\n",
        "    print(\"Final checkpoint path:\", path_sys2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 2 (CoT) student\n",
        "\n",
        "Prompts include CoT instruction; responses should show step-by-step reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    _base2 = ROOT\n",
        "except NameError:\n",
        "    _base2 = Path.cwd()\n",
        "\n",
        "# Use find_checkpoint to locate the actual model files\n",
        "student_path_sys2 = find_checkpoint(str(_base2 / \"distilled-qwen2.5-1.5b-cot\"))\n",
        "if not student_path_sys2 and \"path_sys2\" in dir() and path_sys2:\n",
        "    student_path_sys2 = find_checkpoint(path_sys2)\n",
        "\n",
        "if student_path_sys2:\n",
        "    print(\"Loading checkpoint:\", student_path_sys2)\n",
        "    student_sys2, tok_sys2 = load_student(student_path_sys2)\n",
        "    for p in [\n",
        "        \"A train travels 120 km in 2 hours. If it continues at the same speed, how far will it travel in 5 hours?\",\n",
        "        \"You flip a fair coin 3 times. What is the probability of getting exactly two heads?\",\n",
        "        \"Explain the difference between overfitting and underfitting with an example.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 2 CoT):\", infer_student(student_sys2, tok_sys2, p, mode=\"system2\", max_new_tokens=512))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found at distilled-qwen2.5-1.5b-cot/\")\n",
        "    print(\"Run System 2 distillation first.\")\n",
        "    _p = _base2 / \"distilled-qwen2.5-1.5b-cot\"\n",
        "    if _p.exists():\n",
        "        print(f\"Directory exists but contains: {[f.name for f in _p.iterdir()][:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate System 2 (CoT) student\n",
        "\n",
        "Compute perplexity, BLEU, and ROUGE-L on a held-out sample from the CoT data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if student_path_sys2 and 'student_sys2' in dir():\n",
        "    from distill_app import read_json, evaluate_student\n",
        "    # Use last 50 items from CoT data as eval set\n",
        "    _eval_data_cot = read_json(\"data/omnithought_cot.json\")[-50:]\n",
        "    print(f\"Evaluating System 2 on {len(_eval_data_cot)} held-out samples...\")\n",
        "    eval_results_sys2 = evaluate_student(\n",
        "        student_sys2, tok_sys2, _eval_data_cot, mode=\"system2\", max_new_tokens=512, max_eval=50\n",
        "    )\n",
        "else:\n",
        "    print(\"System 2 student not loaded. Run distillation and test cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4️⃣ Teacher vs student comparison\n",
        "\n",
        "Side-by-side: **Prompt → Teacher | System 1 | System 2**. Missing checkpoints are skipped with a clear message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ce9b72e84dc478c82c62bda55c6a326",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba92eb1fa104f8daec21f82f724b72b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0d98d67da74439aab4c08c840805cdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80f1a06172134de0abba83d8ebf239c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "208ea219ec7b41aab336caffe047e573",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "WARNING:bitsandbytes.backends.cpu.ops:Failed to load CPU gemm_4bit_forward from kernels-community: No module named 'kernels'. Please make sure you already `pip install kernels` and the kernels >= 0.11.1\n",
            "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'npu', 'hpu', 'xpu', 'cuda', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model not found at Qwen/Qwen2.5-7B-Instruct (None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'npu', 'hpu', 'xpu', 'cuda', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)\n",
            "Model not found at /content/distilled-model-research/distilled-qwen2.5-0.5b\n",
            "Model not found at /content/distilled-model-research/distilled-qwen2.5-1.5b-cot\n",
            "\n",
            "============================================================\n",
            "[Prompt 1] Explain what overfitting means.\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 2] What is the time complexity of binary search?\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 3] A train travels 120 km in 2 hours. What is its average speed?\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Prompt 4] Explain the concept of knowledge distillation and why it is useful.\n",
            "============================================================\n",
            "Teacher: (skipped)\n",
            "System 1 student: (not loaded)\n",
            "System 2 student: (not loaded)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "COMPARE_PROMPTS = [\n",
        "    \"Explain what overfitting means.\",\n",
        "    \"What is the time complexity of binary search?\",\n",
        "    \"A train travels 120 km in 2 hours. What is its average speed?\",\n",
        "    \"Explain the concept of knowledge distillation and why it is useful.\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    _base = ROOT\n",
        "except NameError:\n",
        "    _base = Path.cwd()\n",
        "\n",
        "_s1 = next((p for p in [\n",
        "    path_sys1 if \"path_sys1\" in dir() and path_sys1 else None,\n",
        "    str(_base / \"distilled-qwen2.5-0.5b\"),\n",
        "] if p and Path(p).exists()), str(_base / \"distilled-qwen2.5-0.5b\"))\n",
        "\n",
        "_s2 = next((p for p in [\n",
        "    path_sys2 if \"path_sys2\" in dir() and path_sys2 else None,\n",
        "    str(_base / \"distilled-qwen2.5-1.5b-cot\"),\n",
        "] if p and Path(p).exists()), str(_base / \"distilled-qwen2.5-1.5b-cot\"))\n",
        "\n",
        "compare_models(\n",
        "    COMPARE_PROMPTS,\n",
        "    teacher_path=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    system1_path=_s1,\n",
        "    system2_path=_s2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Teacher vs System 2 CoT (side-by-side)\n",
        "\n",
        "Compare teacher and System 2 student on reasoning prompts with CoT-style prompting. Loads teacher and student if not already in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run System 2 distillation first.\n"
          ]
        }
      ],
      "source": [
        "COT_COMPARE = [\n",
        "    \"A bag has 3 red balls and 2 blue balls. If you draw two without replacement, what is the probability both are red?\",\n",
        "    \"What is the derivative of x^3 + 2x^2 - 5x + 7? Explain the steps.\",\n",
        "]\n",
        "if Path(\"./distilled-qwen2.5-1.5b-cot\").exists():\n",
        "    try:\n",
        "        _t, _tt = load_teacher(\"Qwen/Qwen2.5-7B-Instruct\")\n",
        "        _s2, _ts2 = load_student(\"./distilled-qwen2.5-1.5b-cot\")\n",
        "        for p in COT_COMPARE:\n",
        "            print(\"#\" * 72)\n",
        "            print(\"Prompt:\", p)\n",
        "            print(\"\\n[Teacher CoT]\", infer_student(_t, _tt, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print(\"\\n[Student CoT]\", infer_student(_s2, _ts2, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print()\n",
        "    except Exception as e:\n",
        "        print(\"Could not load models:\", e)\n",
        "else:\n",
        "    print(\"Run System 2 distillation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6️⃣ Evaluation Summary\n",
        "\n",
        "Side-by-side metrics for both distilled students."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect results from both evaluations\n",
        "_s1 = eval_results_sys1 if 'eval_results_sys1' in dir() else {}\n",
        "_s2 = eval_results_sys2 if 'eval_results_sys2' in dir() else {}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DISTILLATION EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<20} {'System 1 (0.5B)':>18} {'System 2 (1.5B CoT)':>20}\")\n",
        "print(\"-\" * 60)\n",
        "for metric in ['perplexity', 'bleu', 'rouge_l']:\n",
        "    v1 = _s1.get(metric, 'N/A')\n",
        "    v2 = _s2.get(metric, 'N/A')\n",
        "    print(f\"{metric:<20} {str(v1):>18} {str(v2):>20}\")\n",
        "n1 = _s1.get('num_evaluated', 0)\n",
        "n2 = _s2.get('num_evaluated', 0)\n",
        "print(f\"{'num_evaluated':<20} {str(n1):>18} {str(n2):>20}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5️⃣ Scaling up\n",
        "\n",
        "Once a small run works:\n",
        "- **Data:** Increase slices (e.g. `train[:10000]` System 1, `train[:5000]` System 2).\n",
        "- **Epochs:** Set `NUM_EPOCHS_SYS1` / `NUM_EPOCHS_SYS2` to 2–3.\n",
        "- **Batch size:** Increase in generated configs if VRAM allows.\n",
        "- **System 2 student:** Use `Qwen/Qwen2.5-0.5B-Instruct` if VRAM is tight.\n",
        "- **Relabeling:** Set `RELABEL_WITH_TEACHER = True` for teacher-generated labels (slower, often better)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick single-prompt inference\n",
        "\n",
        "Uncomment and run after you have a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model, tokenizer = load_student(\"./distilled-qwen2.5-0.5b\")\n",
        "# print(infer_student(model, tokenizer, \"Explain what overfitting means.\", mode=\"system1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
