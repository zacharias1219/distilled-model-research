{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen Distillation Lab (System 1 + System 2)\n",
        "\n",
        "**Colab-ready notebook** to distill **Qwen2.5-7B-Instruct** into two smaller students:\n",
        "- **System 1** — instruction-following (7B → 0.5B) via black-box KD on DistilQwen_100k\n",
        "- **System 2** — reasoning / chain-of-thought (1.5B) via SFT-style KD on OmniThought\n",
        "\n",
        "Uses `distill_app.py` for data prep and EasyDistill for training. **Run cells in order** (or *Run all*); **GPU runtime recommended**. Colab: open from GitHub so the repo is cloned; local: open the notebook from the repo root."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0️⃣ Runtime setup\n",
        "\n",
        "Confirm GPU and Python. In Colab: **Runtime → Change runtime type → GPU** (e.g. T4) before running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!nvidia-smi\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 22 11:30:09 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1️⃣ Install dependencies\n",
        "\n",
        "Core libs + **EasyDistill from source**. Clone EasyDistill so the `easydistill` CLI and templates are available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q \"torch>=2.1.0\" \"transformers>=4.36.0\" \"datasets>=2.16.0\" \"accelerate>=0.25.0\" \"sentencepiece>=0.1.99\"\n",
        "!pip install -q bitsandbytes>=0.43.0 tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"/content\").exists():\n",
        "    EASYDIR = Path(\"/content/easydistill\")\n",
        "else:\n",
        "    EASYDIR = Path.cwd() / \"easydistill\"\n",
        "\n",
        "if not EASYDIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/modelscope/easydistill.git\", str(EASYDIR)], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", str(EASYDIR)], check=True)\n",
        "print(\"EasyDistill installed from\", EASYDIR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EasyDistill installed from /content/easydistill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clone repo (if needed)\n",
        "\n",
        "If you got FileNotFoundError above (e.g. opened from Drive/upload): run the code cell below once, then re-run the Project root and imports cell. Set GITHUB_REPO to your fork if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace YOUR_USERNAME with your GitHub username\n",
        "GITHUB_REPO = \"https://github.com/YOUR_USERNAME/distilled-model-research.git\"\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "if Path(\"/content\").exists() and not (Path(\"/content/distilled-model-research\") / \"distill_app.py\").exists():\n",
        "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, \"/content/distilled-model-research\"], check=True)\n",
        "    import os\n",
        "    os.chdir(\"/content/distilled-model-research\")\n",
        "    print(\"Cloned. Now re-run the 'Project root and imports' cell above.\")\n",
        "else:\n",
        "    print(\"Not in Colab or repo already present. If you still see FileNotFoundError, run locally from the repo root.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clone repo (if needed)\n",
        "\n",
        "**Only if you got `FileNotFoundError` above** (e.g. you opened this notebook from Drive or uploaded it). Run this cell once, then re-run the \"Project root and imports\" cell above. Set `GITHUB_REPO` to your fork if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project root and imports\n",
        "\n",
        "Ensure we're in the repo root (where `distill_app.py` lives). In Colab from GitHub, repo is usually `/content/distilled-model-research`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _find_project_root():\n",
        "    if Path(\"/content\").exists():\n",
        "        for d in Path(\"/content\").iterdir():\n",
        "            if d.is_dir() and (d / \"distill_app.py\").exists():\n",
        "                return d\n",
        "    for p in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        if (p / \"distill_app.py\").exists():\n",
        "            return p\n",
        "    return Path.cwd()\n",
        "\n",
        "ROOT = _find_project_root()\n",
        "if ROOT != Path.cwd():\n",
        "    import os\n",
        "    os.chdir(ROOT)\n",
        "    print(\"Working directory:\", ROOT)\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "if not (ROOT / \"distill_app.py\").exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"distill_app.py not found. Colab (Drive/upload): run the 'Clone repo (if needed)' cell below, then re-run this cell. \"\n",
        "        \"Local: run this notebook from the repo root (the folder that contains distill_app.py).\"\n",
        "    )\n",
        "\n",
        "from distill_app import (\n",
        "    load_teacher,\n",
        "    prepare_system1_dataset,\n",
        "    prepare_system2_dataset,\n",
        "    distill_system1,\n",
        "    distill_system2,\n",
        "    compare_models,\n",
        "    load_student,\n",
        "    infer_student,\n",
        "    format_prompt,\n",
        ")\n",
        "print(\"distill_app imported from\", ROOT)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "distill_app.py not found. Colab: open this notebook from GitHub (File → Open notebook → GitHub) so the repo is cloned. Local: run from the repo root.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1716981085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"distill_app.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"distill_app.py not found. Colab: open this notebook from GitHub (File → Open notebook → GitHub) so the repo is cloned. Local: run from the repo root.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: distill_app.py not found. Colab: open this notebook from GitHub (File → Open notebook → GitHub) so the repo is cloned. Local: run from the repo root."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports are in the \"Project root and imports\" cell above. Skip this cell."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2️⃣ System 1: Instruction-following distillation (7B → 0.5B)\n",
        "\n",
        "Load a subset of **DistilQwen_100k**, optionally re-label with the teacher, then run black-box KD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config\n",
        "\n",
        "Increase `DATASET_SLICE_SYS1` (e.g. `train[:5000]`) or `NUM_EPOCHS_SYS1` for better quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TEACHER_MODEL_SYS1 = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "STUDENT_MODEL_SYS1 = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "DATASET_SLICE_SYS1 = \"train[:1000]\"\n",
        "NUM_EPOCHS_SYS1 = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data & Label (Optional)\n",
        "\n",
        "Loads DistilQwen_100k, maps to `{instruction, input, output}`. Set `RELABEL_WITH_TEACHER = True` to re-generate outputs with the teacher (slower, more VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RELABEL_WITH_TEACHER = False  # Set True to re-label with teacher (requires loading teacher first)\n",
        "\n",
        "teacher_sys1 = None\n",
        "tokenizer_sys1 = None\n",
        "if RELABEL_WITH_TEACHER:\n",
        "    teacher_sys1, tokenizer_sys1 = load_teacher(TEACHER_MODEL_SYS1)\n",
        "\n",
        "prepare_system1_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS1,\n",
        "    teacher_model=teacher_sys1,\n",
        "    teacher_tokenizer=tokenizer_sys1,\n",
        "    relabel_with_teacher=RELABEL_WITH_TEACHER,\n",
        "    out_instructions=\"data/train_instructions.json\",\n",
        "    out_labeled=\"data/train_labeled.json\",\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prepare_system1_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-415109321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mteacher_sys1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_sys1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEACHER_MODEL_SYS1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m prepare_system1_dataset(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mslice_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_SLICE_SYS1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_sys1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prepare_system1_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Distillation\n",
        "\n",
        "Calls EasyDistill (black-box KD). Checkpoint will be written to `./distilled-qwen2.5-0.5b` (or the path you set in config)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config_sys1 = {\n",
        "    \"teacher_model\": TEACHER_MODEL_SYS1,\n",
        "    \"student_model\": STUDENT_MODEL_SYS1,\n",
        "    \"labeled_path\": \"data/train_labeled.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS1,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-0.5b\",\n",
        "    \"config_path\": \"configs/kd_black_box_qwen_0_5b.json\",\n",
        "    \"template_path\": None,\n",
        "}\n",
        "\n",
        "# If EasyDistill was cloned, point to its template (configs/chat_template/chat_template_kd.jinja)\n",
        "if Path(\"/content\").exists() and Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = \"/content/easydistill/configs/chat_template/chat_template_kd.jinja\"\n",
        "elif (Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\").exists():\n",
        "    config_sys1[\"template_path\"] = str(Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\")\n",
        "\n",
        "path_sys1 = distill_system1(config_sys1)\n",
        "if path_sys1:\n",
        "    print(\"Final checkpoint path:\", path_sys1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'distill_system1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1598478886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig_sys1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"template_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"easydistill\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"configs\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"chat_template_kd.jinja\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpath_sys1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistill_system1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final checkpoint path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_sys1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'distill_system1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 1 student\n",
        "\n",
        "Load the distilled model and run a few prompts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "student_path_sys1 = \"./distilled-qwen2.5-0.5b\"\n",
        "if Path(student_path_sys1).exists():\n",
        "    student_sys1, tok_sys1 = load_student(student_path_sys1)\n",
        "    for p in [\n",
        "        \"Explain what a large language model is to a high school student.\",\n",
        "        \"Write a Python function to check if a number is prime.\",\n",
        "        \"Give me three use cases of knowledge distillation in deep learning.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 1):\", infer_student(student_sys1, tok_sys1, p, mode=\"system1\", max_new_tokens=256))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found — run System 1 distillation first.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint not found — run System 1 distillation first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## System 2 Distillation (Reasoning / CoT)\n",
        "\n",
        "Train a CoT-capable student on OmniThought so it shows step-by-step reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STUDENT_MODEL_SYS2 = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "DATASET_SLICE_SYS2 = \"train[:2000]\"\n",
        "RV_MIN = 0.6\n",
        "CD_MIN = 0.6\n",
        "NUM_EPOCHS_SYS2 = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare CoT Data\n",
        "\n",
        "Load OmniThought, filter by RV/CD if present, map to `{instruction, output=cot}` and save to `data/omnithought_cot.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prepare_system2_dataset(\n",
        "    slice_str=DATASET_SLICE_SYS2,\n",
        "    rv_min=RV_MIN,\n",
        "    cd_min=CD_MIN,\n",
        "    out_cot=\"data/omnithought_cot.json\",\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prepare_system2_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3587371658.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prepare_system2_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mslice_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_SLICE_SYS2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrv_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRV_MIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcd_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCD_MIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mout_cot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/omnithought_cot.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prepare_system2_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run CoT Distillation\n",
        "\n",
        "Calls EasyDistill (kd_black_box_train_only). Checkpoint: `./distilled-qwen2.5-1.5b-cot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config_sys2 = {\n",
        "    \"student_model\": STUDENT_MODEL_SYS2,\n",
        "    \"cot_path\": \"data/omnithought_cot.json\",\n",
        "    \"num_epochs\": NUM_EPOCHS_SYS2,\n",
        "    \"out_dir\": \"./distilled-qwen2.5-1.5b-cot\",\n",
        "    \"config_path\": \"configs/kd_cot_qwen_1_5b.json\",\n",
        "}\n",
        "# Use EasyDistill template from clone (same as System 1)\n",
        "_tpl = Path(\"/content/easydistill/configs/chat_template/chat_template_kd.jinja\") if Path(\"/content\").exists() else Path.cwd() / \"easydistill\" / \"configs\" / \"chat_template\" / \"chat_template_kd.jinja\"\n",
        "if _tpl.exists():\n",
        "    config_sys2[\"template_path\"] = str(_tpl)\n",
        "\n",
        "path_sys2 = distill_system2(config_sys2)\n",
        "if path_sys2:\n",
        "    print(\"Final checkpoint path:\", path_sys2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'distill_system2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3524631506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mconfig_sys2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"template_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpath_sys2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistill_system2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_sys2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_sys2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final checkpoint path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_sys2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'distill_system2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test System 2 (CoT) student\n",
        "\n",
        "Prompts include CoT instruction; responses should show step-by-step reasoning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "student_path_sys2 = \"./distilled-qwen2.5-1.5b-cot\"\n",
        "if Path(student_path_sys2).exists():\n",
        "    student_sys2, tok_sys2 = load_student(student_path_sys2)\n",
        "    for p in [\n",
        "        \"A train travels 120 km in 2 hours. If it continues at the same speed, how far will it travel in 5 hours?\",\n",
        "        \"You flip a fair coin 3 times. What is the probability of getting exactly two heads?\",\n",
        "        \"Explain the difference between overfitting and underfitting with an example.\",\n",
        "    ]:\n",
        "        print(\"=\" * 72)\n",
        "        print(\"Prompt:\", p)\n",
        "        print(\"Student (System 2 CoT):\", infer_student(student_sys2, tok_sys2, p, mode=\"system2\", max_new_tokens=512))\n",
        "        print()\n",
        "else:\n",
        "    print(\"Checkpoint not found — run System 2 distillation first.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint not found — run System 2 distillation first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4️⃣ Teacher vs student comparison\n",
        "\n",
        "Side-by-side: **Prompt → Teacher | System 1 | System 2**. Missing checkpoints are skipped with a clear message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "COMPARE_PROMPTS = [\n",
        "    \"Explain what overfitting means.\",\n",
        "    \"What is the time complexity of binary search?\",\n",
        "    \"A train travels 120 km in 2 hours. What is its average speed?\",\n",
        "    \"Explain the concept of knowledge distillation and why it is useful.\",\n",
        "]\n",
        "\n",
        "compare_models(\n",
        "    COMPARE_PROMPTS,\n",
        "    teacher_path=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    system1_path=\"./distilled-qwen2.5-0.5b\",\n",
        "    system2_path=\"./distilled-qwen2.5-1.5b-cot\",\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'compare_models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-631455379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m compare_models(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mCOMPARE_PROMPTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mteacher_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Qwen/Qwen2.5-7B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'compare_models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Teacher vs System 2 CoT (side-by-side)\n",
        "\n",
        "Compare teacher and System 2 student on reasoning prompts with CoT-style prompting. Loads teacher and student if not already in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "COT_COMPARE = [\n",
        "    \"A bag has 3 red balls and 2 blue balls. If you draw two without replacement, what is the probability both are red?\",\n",
        "    \"What is the derivative of x^3 + 2x^2 - 5x + 7? Explain the steps.\",\n",
        "]\n",
        "if Path(\"./distilled-qwen2.5-1.5b-cot\").exists():\n",
        "    try:\n",
        "        _t, _tt = load_teacher(\"Qwen/Qwen2.5-7B-Instruct\")\n",
        "        _s2, _ts2 = load_student(\"./distilled-qwen2.5-1.5b-cot\")\n",
        "        for p in COT_COMPARE:\n",
        "            print(\"#\" * 72)\n",
        "            print(\"Prompt:\", p)\n",
        "            print(\"\\n[Teacher CoT]\", infer_student(_t, _tt, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print(\"\\n[Student CoT]\", infer_student(_s2, _ts2, p, mode=\"system2\", max_new_tokens=512)[:1000])\n",
        "            print()\n",
        "    except Exception as e:\n",
        "        print(\"Could not load models:\", e)\n",
        "else:\n",
        "    print(\"Run System 2 distillation first.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run System 2 distillation first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5️⃣ Scaling up\n",
        "\n",
        "Once a small run works:\n",
        "- **Data:** Increase slices (e.g. `train[:10000]` System 1, `train[:5000]` System 2).\n",
        "- **Epochs:** Set `NUM_EPOCHS_SYS1` / `NUM_EPOCHS_SYS2` to 2–3.\n",
        "- **Batch size:** Increase in generated configs if VRAM allows.\n",
        "- **System 2 student:** Use `Qwen/Qwen2.5-0.5B-Instruct` if VRAM is tight.\n",
        "- **Relabeling:** Set `RELABEL_WITH_TEACHER = True` for teacher-generated labels (slower, often better)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick single-prompt inference\n",
        "\n",
        "Uncomment and run after you have a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model, tokenizer = load_student(\"./distilled-qwen2.5-0.5b\")\n",
        "# print(infer_student(model, tokenizer, \"Explain what overfitting means.\", mode=\"system1\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}